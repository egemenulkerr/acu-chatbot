# ============================================================================
# backend/app/services/web_scraper/duyurular_scraper.py - Duyurular Scraper
# ============================================================================

import logging
import requests
from bs4 import BeautifulSoup
from datetime import datetime

logger = logging.getLogger(__name__)

DUYURULAR_URL = "https://www.artvin.edu.tr/tr/duyurular"
MAX_DUYURU = 5


def scrape_announcements() -> str | None:
    """
    AÃ‡Ãœ duyurular sayfasÄ±ndan son MAX_DUYURU kadar duyuruyu Ã§eker.
    Hata durumunda None dÃ¶ner.
    """
    try:
        logger.info("Duyurular sayfasÄ± taranÄ±yor...")
        r = requests.get(DUYURULAR_URL, timeout=10)
        if r.status_code != 200:
            logger.error(f"Duyurular sayfasÄ±na ulaÅŸÄ±lamadÄ±: {r.status_code}")
            return None

        soup = BeautifulSoup(r.content, "html.parser")

        # Tipik AÃ‡Ãœ site yapÄ±sÄ±: duyurular liste veya article elemanlarÄ±nda
        items = []

        # YÃ¶ntem 1: ul/li yapÄ±sÄ±ndaki duyurular
        news_list = soup.find("ul", class_=lambda c: c and "news" in c.lower())
        if news_list:
            for li in news_list.find_all("li")[:MAX_DUYURU]:
                a = li.find("a")
                if a:
                    title = a.get_text(strip=True)
                    href = a.get("href", "")
                    if href and not href.startswith("http"):
                        href = "https://www.artvin.edu.tr" + href
                    if title:
                        items.append((title, href))

        # YÃ¶ntem 2: article/div yapÄ±sÄ±
        if not items:
            for article in soup.find_all(["article", "div"], class_=lambda c: c and any(
                k in c.lower() for k in ["duyuru", "news", "haber", "item", "post"]
            ))[:MAX_DUYURU]:
                a = article.find("a")
                if a:
                    title = a.get_text(strip=True)
                    href = a.get("href", "")
                    if href and not href.startswith("http"):
                        href = "https://www.artvin.edu.tr" + href
                    if title and len(title) > 5:
                        items.append((title, href))

        # YÃ¶ntem 3: Sayfa genelinde link listesi
        if not items:
            for a in soup.find_all("a", href=True)[:30]:
                href = a.get("href", "")
                title = a.get_text(strip=True)
                if (
                    "duyuru" in href.lower() or "haber" in href.lower()
                ) and title and len(title) > 10:
                    if not href.startswith("http"):
                        href = "https://www.artvin.edu.tr" + href
                    items.append((title, href))
                    if len(items) >= MAX_DUYURU:
                        break

        if not items:
            logger.warning("Duyuru bulunamadÄ±.")
            return None

        today = datetime.now().strftime("%d.%m.%Y")
        lines = [f"ðŸ“¢ **Son Duyurular** ({today})\n"]
        for i, (title, href) in enumerate(items[:MAX_DUYURU], 1):
            lines.append(f"{i}. {title}\n   {href}")

        lines.append(f"\nðŸ”— TÃ¼m duyurular: {DUYURULAR_URL}")
        logger.info(f"{len(items)} duyuru Ã§ekildi.")
        return "\n".join(lines)

    except Exception as e:
        logger.error(f"Duyurular scraper hatasÄ±: {e}", exc_info=True)
        return None
