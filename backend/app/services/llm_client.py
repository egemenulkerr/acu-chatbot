# ============================================================================
# backend/app/services/llm_client.py - Google Gemini API Client
# ============================================================================
# AÃ§Ä±klama:
#   Google Generative AI (Gemini) API'sini kullanarak sohbet cevaplarÄ±
#   oluÅŸturur. Model Ã¶rneÄŸi module seviyesinde cache'lenir â€” her istekte
#   yeniden oluÅŸturulmaz. KonuÅŸma geÃ§miÅŸini destekler.
# ============================================================================

import os
import logging
from typing import Optional

from dotenv import load_dotenv
import google.generativeai as genai


# ============================================================================
# CONFIGURATION
# ============================================================================

logger: logging.Logger = logging.getLogger(__name__)

load_dotenv()
GOOGLE_API_KEY: Optional[str] = os.getenv("GOOGLE_API_KEY")

SYSTEM_PROMPT: str = """
Sen Artvin Ã‡oruh Ãœniversitesi (AÃ‡Ãœ) asistanÄ±sÄ±n.
KullanÄ±cÄ±larÄ±n akademik, idari ve kampÃ¼s-ilgili sorularÄ±na cevap verirsin.
Samimi, yardÄ±msever ve kÄ±sa cevaplar ver.
EÄŸer sorunun konu dÄ±ÅŸÄ± ise nazikÃ§e konuya dÃ¶ndÃ¼rmeye Ã§alÄ±ÅŸ.
"""

# Module-level singleton â€” model yalnÄ±zca bir kez baÅŸlatÄ±lÄ±r
_CACHED_MODEL: Optional[genai.GenerativeModel] = None


# ============================================================================
# MODEL INITIALIZATION (CACHED)
# ============================================================================

def _get_model() -> Optional[genai.GenerativeModel]:
    """
    Gemini model Ã¶rneÄŸini dÃ¶ndÃ¼r. Ä°lk Ã§aÄŸrÄ±da baÅŸlatÄ±r, sonrasÄ±nda cache'den verir.
    """
    global _CACHED_MODEL

    if _CACHED_MODEL is not None:
        return _CACHED_MODEL

    if not GOOGLE_API_KEY:
        logger.error("âŒ GOOGLE_API_KEY environment variable eksik!")
        return None

    try:
        genai.configure(api_key=GOOGLE_API_KEY)

        logger.info("ğŸ” Gemini modelleri aranÄ±yor...")
        available_models = list(genai.list_models())

        gemini_models = [
            m for m in available_models
            if 'generateContent' in m.supported_generation_methods
            and 'gemini' in m.name
        ]

        model_name = "models/gemini-1.5-flash"

        for m in gemini_models:
            if 'flash' in m.name:
                model_name = m.name
                break
        else:
            for m in gemini_models:
                if 'pro' in m.name:
                    model_name = m.name
                    break
            else:
                if gemini_models:
                    model_name = gemini_models[0].name

        logger.info(f"âœ… Gemini modeli seÃ§ildi ve cache'lendi: {model_name}")
        _CACHED_MODEL = genai.GenerativeModel(
            model_name,
            system_instruction=SYSTEM_PROMPT
        )
        return _CACHED_MODEL

    except Exception as e:
        logger.error(f"âŒ Model baÅŸlatma hatasÄ±: {e}", exc_info=True)
        return None


# ============================================================================
# MAIN LLM FUNCTION
# ============================================================================

def stream_llm_response(user_message: str, history: list[dict] | None = None):
    """
    Gemini cevabÄ±nÄ± token token yield eden sync generator.
    asyncio.to_thread + asyncio.Queue ile async endpoint'lerden kullanÄ±lmalÄ±.
    """
    model = _get_model()
    if not model:
        yield "âš™ï¸ AI servisi baÅŸlatÄ±lamadÄ±."
        return

    try:
        gemini_history = []
        if history:
            for msg in history[-10:]:
                role = "user" if msg.get("role") == "user" else "model"
                text = msg.get("text", "").strip()
                if text:
                    gemini_history.append({"role": role, "parts": [text]})

        chat = model.start_chat(history=gemini_history)
        response = chat.send_message(user_message, stream=True)
        for chunk in response:
            if chunk.text:
                yield chunk.text
    except Exception as e:
        logger.error(f"âŒ Streaming LLM HatasÄ±: {e}", exc_info=True)
        yield "ÃœzgÃ¼nÃ¼m, ÅŸu anda AI servisine baÄŸlanamÄ±yorum."


def get_llm_response(user_message: str, history: list[dict] | None = None) -> str:
    """
    KullanÄ±cÄ± mesajÄ±nÄ± Gemini'ye gÃ¶nder ve cevap al.

    Bu fonksiyon sync'tir â€” async endpoint'lerden asyncio.to_thread ile Ã§aÄŸrÄ±lmalÄ±.

    Args:
        user_message : KullanÄ±cÄ±nÄ±n son mesajÄ±
        history      : [{role: "user"|"bot", text: "..."}] formatÄ±nda geÃ§miÅŸ

    Returns:
        str: Gemini'nin yanÄ±tÄ± veya hata mesajÄ±
    """
    model = _get_model()

    if not model:
        return "âš™ï¸ Sistem yapÄ±landÄ±rma hatasÄ±: AI servisi baÅŸlatÄ±lamadÄ±. LÃ¼tfen yÃ¶neticiye baÅŸvurun."

    try:
        # GeÃ§miÅŸi Gemini'nin beklediÄŸi formata dÃ¶nÃ¼ÅŸtÃ¼r
        gemini_history = []
        if history:
            for msg in history[-10:]:  # Son 5 tur (10 mesaj)
                role = "user" if msg.get("role") == "user" else "model"
                text = msg.get("text", "").strip()
                if text:
                    gemini_history.append({"role": role, "parts": [text]})

        chat = model.start_chat(history=gemini_history)
        response = chat.send_message(user_message)
        response_text = response.text.strip()

        logger.info(f"âœ… LLM yanÄ±tÄ± oluÅŸturuldu ({len(response_text)} karakter)")
        return response_text

    except Exception as e:
        logger.error(f"âŒ LLM HatasÄ±: {e}", exc_info=True)
        return "ÃœzgÃ¼nÃ¼m, ÅŸu anda AI servisine baÄŸlanamÄ±yorum. LÃ¼tfen daha sonra tekrar deneyin."
